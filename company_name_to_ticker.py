#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Company Name to Ticker Converter - FIXED to match test3.py logic exactly
ä¿®å¤ç‰ˆæœ¬ï¼Œå®Œå…¨åŒ¹é…test3.pyçš„é€»è¾‘
"""

import json
import pandas as pd
import os
import re
import requests
import time
import urllib.parse
from pathlib import Path
from difflib import SequenceMatcher
from typing import Optional, Dict, List, Tuple


class EnhancedTest3TickerConverter:
    def __init__(self,
                 company_tickers_file="company_tickers.json",
                 company_tickers_exchange_file="company_tickers_exchange.json"):
        """
        Initialize the enhanced test3.py logic ticker converter
        """
        self.companies_data = []

        # Load local SEC data - ä½¿ç”¨ä¸test3.pyå®Œå…¨ç›¸åŒçš„åŠ è½½é€»è¾‘
        self.load_local_data(company_tickers_exchange_file, company_tickers_file)

        # Company suffixes from test3.py
        self.company_suffixes = [
            'INC', 'CORP', 'CORPORATION', 'LTD', 'LIMITED', 'LLC', 'LP', 'LLP',
            'CO', 'COMPANY', 'HOLDINGS', 'GROUP', 'ENTERPRISES', 'SYSTEMS',
            'TECHNOLOGIES', 'TECH', 'SOLUTIONS', 'SERVICES', 'INTERNATIONAL',
            'PLC', 'SA', 'NV', 'AG', 'GMBH', 'SPA', 'BV', 'NEW'  # Added NEW from test3.py
        ]

    def load_local_data(self, json_file1: str, json_file2: str):
        """
        åŠ è½½æœ¬åœ°JSONæ–‡ä»¶æ•°æ® - å®Œå…¨ä½¿ç”¨test3.pyçš„é€»è¾‘
        json_file1: company_tickers_exchange.json
        json_file2: company_tickers.json
        """
        # åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶ (company_tickers_exchange.json)
        try:
            with open(json_file1, 'r', encoding='utf-8') as f:
                data1 = json.load(f)
                if 'data' in data1 and 'fields' in data1:
                    fields = data1['fields']
                    name_idx = fields.index('name')
                    ticker_idx = fields.index('ticker')
                    for row in data1['data']:
                        self.companies_data.append({
                            'name': row[name_idx],
                            'ticker': row[ticker_idx],
                            'source': 'file1'
                        })
        except FileNotFoundError:
            print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°æ–‡ä»¶ {json_file1}")
        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶ {json_file1} æ—¶å‡ºé”™: {e}")

        # åŠ è½½ç¬¬äºŒä¸ªæ–‡ä»¶ (company_tickers.json)
        try:
            with open(json_file2, 'r', encoding='utf-8') as f:
                data2 = json.load(f)
                for key, company in data2.items():
                    if isinstance(company, dict) and 'title' in company and 'ticker' in company:
                        existing_tickers = [comp['ticker'] for comp in self.companies_data]
                        if company['ticker'] not in existing_tickers:
                            self.companies_data.append({
                                'name': company['title'],
                                'ticker': company['ticker'],
                                'source': 'file2'
                            })
        except FileNotFoundError:
            print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°æ–‡ä»¶ {json_file2}")
        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶ {json_file2} æ—¶å‡ºé”™: {e}")

        print(f"æˆåŠŸåŠ è½½ {len(self.companies_data)} å®¶å…¬å¸çš„æ•°æ®")

    def extract_core_keywords(self, company_name: str) -> List[str]:
        """æå–å…¬å¸åç§°ä¸­çš„æ ¸å¿ƒå…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆï¼Œç”¨äºåœ¨çº¿æœç´¢ï¼‰"""
        name = company_name.upper().strip()

        # ç§»é™¤å…¬å¸åç¼€
        for suffix in self.company_suffixes:
            name = re.sub(rf'\b{suffix}\b', '', name)

        # æ¸…ç†æ ‡ç‚¹ç¬¦å·
        name = re.sub(r'[.,&\-/()]', ' ', name)
        name = re.sub(r'\s+', ' ', name).strip()

        # åˆ†å‰²å¹¶è¿‡æ»¤çŸ­è¯å’Œå¸¸è§è¯
        stop_words = {'THE', 'OF', 'AND', 'OR', 'FOR', 'WITH', 'A', 'AN', 'AT', 'BY', 'IN', 'ON'}
        keywords = [word for word in name.split()
                    if len(word) >= 2 and word not in stop_words]  # é™ä½æœ€å°é•¿åº¦åˆ°2

        return keywords

    def calculate_company_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—å…¬å¸åç§°çš„ç›¸ä¼¼åº¦ï¼ˆé‡‡ç”¨test3.pyçš„é€»è¾‘ï¼‰"""
        norm1 = self.normalize_company_name(name1)
        norm2 = self.normalize_company_name(name2)

        # å®Œå…¨åŒ¹é…
        if norm1 == norm2:
            return 1.0

        # é¿å…é”™è¯¯çš„éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚ ALLERGAN å’Œ ARGANï¼‰
        # æ£€æŸ¥æ˜¯å¦ä¸€ä¸ªæ˜¯å¦å¦ä¸€ä¸ªçš„çœŸæ­£å­ä¸²
        if norm1 and norm2:
            # å¦‚æœä¸¤ä¸ªåç§°é•¿åº¦å·®å¼‚å¾ˆå¤§ï¼Œé™ä½ç›¸ä¼¼åº¦
            len_diff = abs(len(norm1) - len(norm2))
            if len_diff > max(len(norm1), len(norm2)) * 0.3:  # é•¿åº¦å·®å¼‚è¶…è¿‡30%
                base_ratio = SequenceMatcher(None, norm1, norm2).ratio()
                # å¯¹é•¿åº¦å·®å¼‚å¾ˆå¤§çš„æƒ…å†µè¿›è¡Œæƒ©ç½š
                return base_ratio * (1 - len_diff / max(len(norm1), len(norm2)))

            # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„åŒ…å«å…³ç³»
            if norm1 in norm2 or norm2 in norm1:
                shorter = norm1 if len(norm1) < len(norm2) else norm2
                longer = norm2 if len(norm1) < len(norm2) else norm1

                # å¦‚æœè¾ƒçŸ­çš„åç§°å è¾ƒé•¿åç§°çš„æ¯”ä¾‹å¾ˆé«˜ï¼Œæ‰è®¤ä¸ºæ˜¯åŒ…å«å…³ç³»
                if len(shorter) >= len(longer) * 0.7:
                    return 0.9
                else:
                    # å¦åˆ™é™ä½ç›¸ä¼¼åº¦
                    return SequenceMatcher(None, norm1, norm2).ratio() * 0.8

        # åºåˆ—åŒ¹é…
        return SequenceMatcher(None, norm1, norm2).ratio()

    def normalize_company_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–å…¬å¸åç§°ï¼ˆé‡‡ç”¨test3.pyçš„é€»è¾‘ï¼‰"""
        name = name.upper().strip()
        name = re.sub(r'[.,&\-/]', ' ', name)
        name = re.sub(r'\s+', ' ', name)

        words = name.split()
        filtered_words = []
        for word in words:
            if word not in self.company_suffixes:
                filtered_words.append(word)

        return ' '.join(filtered_words).strip()

    def search_local(self, company_name: str, threshold: float = 0.75) -> List[Tuple[Dict, float]]:
        """
        åœ¨æœ¬åœ°æ•°æ®ä¸­æœç´¢ - å®Œå…¨ä½¿ç”¨test3.pyçš„é€»è¾‘
        """
        results = []

        for company in self.companies_data:
            similarity = self.calculate_company_similarity(company_name, company['name'])
            if similarity >= threshold:
                results.append((company, similarity))

        results.sort(key=lambda x: x[1], reverse=True)

        # test3.pyçš„é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æœ€ä½³åŒ¹é…æ˜¯å¦çœŸçš„åˆç†
        if results:
            best_match = results[0]
            best_similarity = best_match[1]
            best_company = best_match[0]

            # è¿›è¡Œå…³é”®è¯æ£€æŸ¥éªŒè¯
            user_keywords = set(self.extract_core_keywords(company_name))
            match_keywords = set(self.extract_core_keywords(best_company['name']))

            # è®¡ç®—å…³é”®è¯é‡å 
            intersection = user_keywords.intersection(match_keywords)

            # å¦‚æœæ²¡æœ‰ä»»ä½•å…³é”®è¯é‡å ï¼Œä½†ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè¿™å¯èƒ½æ˜¯é”™è¯¯åŒ¹é…
            if len(intersection) == 0 and best_similarity > 0.8:
                print(f"è­¦å‘Šï¼šé«˜ç›¸ä¼¼åº¦ä½†æ— å…³é”®è¯é‡å ï¼Œå¯èƒ½æ˜¯é”™è¯¯åŒ¹é…")
                print(f"ç”¨æˆ·å…³é”®è¯: {user_keywords}")
                print(f"åŒ¹é…å…³é”®è¯: {match_keywords}")
                print(f"è·³è¿‡å¯ç–‘åŒ¹é…: {best_company['name']} (ç›¸ä¼¼åº¦: {best_similarity:.2f})")
                return []

            # å¦‚æœåªæœ‰ä¸€ä¸ªå­—ç¬¦çš„é‡å ï¼Œä¹Ÿè¦è°¨æ…
            if len(intersection) == 0:
                # æ£€æŸ¥å­—ç¬¦çº§åˆ«çš„ç›¸ä¼¼åº¦æ˜¯å¦åˆç†
                norm1 = self.normalize_company_name(company_name)
                norm2 = self.normalize_company_name(best_company['name'])

                # è®¡ç®—é•¿åº¦å·®å¼‚
                len_diff = abs(len(norm1) - len(norm2))
                max_len = max(len(norm1), len(norm2))
                len_similarity = 1 - (len_diff / max_len) if max_len > 0 else 0

                # å¦‚æœé•¿åº¦å·®å¼‚å¤ªå¤§ï¼Œä¹Ÿè®¤ä¸ºæ˜¯å¯ç–‘åŒ¹é…
                if len_similarity < 0.5 and best_similarity > 0.7:
                    print(f"è­¦å‘Šï¼šé•¿åº¦å·®å¼‚è¿‡å¤§çš„å¯ç–‘åŒ¹é…")
                    print(f"'{norm1}' vs '{norm2}'")
                    print(f"é•¿åº¦ç›¸ä¼¼åº¦: {len_similarity:.2f}, æ€»ç›¸ä¼¼åº¦: {best_similarity:.2f}")
                    return []

        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæœ€ä½³åŒ¹é…çš„ç›¸ä¼¼åº¦ä¸å¤Ÿé«˜ï¼Œå¯èƒ½æ˜¯é”™è¯¯åŒ¹é…
        if results and results[0][1] < 0.85:
            print(f"è­¦å‘Šï¼šæœ€ä½³æœ¬åœ°åŒ¹é…ç›¸ä¼¼åº¦è¾ƒä½ ({results[0][1]:.2f})ï¼Œå¯èƒ½ä¸å‡†ç¡®")
            if results[0][1] < 0.80:
                print("ç›¸ä¼¼åº¦è¿‡ä½ï¼Œè·³è¿‡æœ¬åœ°åŒ¹é…ï¼Œç›´æ¥è¿›è¡Œåœ¨çº¿æœç´¢")
                return []

        return results

    def validate_ticker_with_company_verification(self, ticker: str, company_name: str) -> bool:
        """é€šè¿‡åœ¨çº¿éªŒè¯è‚¡ç¥¨ä»£ç ä¸å…¬å¸çš„åŒ¹é…æ€§ - åŠ å¼ºç‰ˆ"""
        try:
            # ä½¿ç”¨Yahoo FinanceéªŒè¯
            yahoo_url = f"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = requests.get(yahoo_url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'chart' in data and data['chart']['result']:
                    result = data['chart']['result'][0]
                    meta = result.get('meta', {})
                    symbol_name = meta.get('longName', '') or meta.get('shortName', '')

                    if symbol_name:
                        similarity = self.calculate_company_similarity(company_name, symbol_name)
                        print(f"YahooéªŒè¯: {ticker} -> {symbol_name} (ç›¸ä¼¼åº¦: {similarity:.2f})")

                        # å¤§å¹…æé«˜éªŒè¯é˜ˆå€¼ï¼Œé¿å…é”™è¯¯åŒ¹é…
                        if similarity > 0.75:  # ä»0.6æé«˜åˆ°0.75
                            return True
                        else:
                            print(f"ç›¸ä¼¼åº¦è¿‡ä½({similarity:.2f})ï¼ŒYahooéªŒè¯å¤±è´¥")
                            return False

                        # å¯¹äºé€€å¸‚è‚¡ç¥¨ï¼ŒYahooå¯èƒ½è¿”å›ä¸å®Œæ•´ä¿¡æ¯ï¼Œè¿›è¡Œé¢å¤–æ£€æŸ¥
                        if not symbol_name or len(symbol_name) < 5:
                            print(f"Yahooè¿”å›ä¿¡æ¯ä¸å®Œæ•´ï¼Œå°è¯•å…¶ä»–éªŒè¯æ–¹æ³•")
                            return self.strict_fallback_verification(ticker, company_name)
                    else:
                        # æ²¡æœ‰å…¬å¸åç§°ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯é€€å¸‚è‚¡ç¥¨
                        print(f"Yahooæ— å…¬å¸åç§°ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯é€€å¸‚è‚¡ç¥¨: {ticker}")
                        return self.strict_fallback_verification(ticker, company_name)

            # YahooéªŒè¯å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨éªŒè¯
            return self.strict_fallback_verification(ticker, company_name)

        except Exception as e:
            print(f"è‚¡ç¥¨ä»£ç éªŒè¯é”™è¯¯: {e}")
            # éªŒè¯å¤±è´¥æ—¶ï¼Œå¦‚æœtickeræ ¼å¼æ­£ç¡®ä¸”åœ¨åˆç†ä¸Šä¸‹æ–‡ä¸­ï¼Œä¹Ÿå¯èƒ½æ˜¯æœ‰æ•ˆçš„
            return self.strict_fallback_verification(ticker, company_name)

    def strict_fallback_verification(self, ticker: str, company_name: str) -> bool:
        """ä¸¥æ ¼çš„å¤‡ç”¨éªŒè¯æ–¹æ³•"""
        try:
            # æ–¹æ³•1: æ£€æŸ¥tickeræ˜¯å¦ä¸ºå¸¸è§çš„å·²çŸ¥è‚¡ç¥¨ä»£ç æ ¼å¼
            if len(ticker) >= 2 and ticker.isalpha() and ticker.isupper():
                # ä¸¥æ ¼æ£€æŸ¥æ˜¯å¦åœ¨å¸¸è§çš„é”™è¯¯åˆ—è¡¨ä¸­
                common_false_positives = {
                    'THE', 'AND', 'FOR', 'WITH', 'FROM', 'HTML', 'HTTP', 'HTTPS',
                    'NEWS', 'INFO', 'HELP', 'MORE', 'ABOUT', 'CONTACT', 'HOME',
                    'MAIN', 'MENU', 'SEARCH', 'LOGIN', 'PAGE', 'SITE', 'LINK',
                    'NYSE', 'NASDAQ', 'NASDA', 'NASD', 'MKT', 'COM', 'ORG', 'NET'
                }
                if ticker in common_false_positives:
                    print(f"tickeråœ¨é”™è¯¯åˆ—è¡¨ä¸­ï¼Œå¤‡ç”¨éªŒè¯å¤±è´¥: {ticker}")
                    return False

                # è¿›è¡ŒåŸºæœ¬çš„å­—ç¬¦åŒ¹é…æ£€æŸ¥
                company_chars = set(self.normalize_company_name(company_name).replace(' ', ''))
                ticker_chars = set(ticker)

                # æ£€æŸ¥tickerçš„å­—ç¬¦æ˜¯å¦å¤§éƒ¨åˆ†éƒ½èƒ½åœ¨å…¬å¸åç§°ä¸­æ‰¾åˆ°
                matching_chars = ticker_chars.intersection(company_chars)
                if len(matching_chars) >= len(ticker_chars) * 0.6:  # è‡³å°‘60%çš„å­—ç¬¦åŒ¹é…
                    print(f"å­—ç¬¦åŒ¹é…éªŒè¯é€šè¿‡: {ticker} (åŒ¹é…å­—ç¬¦: {matching_chars})")
                    return True
                else:
                    print(f"å­—ç¬¦åŒ¹é…éªŒè¯å¤±è´¥: {ticker} (åŒ¹é…å­—ç¬¦: {matching_chars}, éœ€è¦: {ticker_chars})")
                    return False

            # æ–¹æ³•2: å°è¯•Alpha VantageéªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            time.sleep(1)
            return self.verify_with_alpha_vantage(ticker, company_name)

        except Exception as e:
            print(f"ä¸¥æ ¼å¤‡ç”¨éªŒè¯é”™è¯¯: {e}")
            return False

    def verify_with_alpha_vantage(self, ticker: str, company_name: str) -> bool:
        """ä½¿ç”¨Alpha VantageéªŒè¯"""
        try:
            # ä½¿ç”¨demo keyè¿›è¡ŒåŸºæœ¬éªŒè¯
            search_url = "https://www.alphavantage.co/query"
            params = {
                'function': 'SYMBOL_SEARCH',
                'keywords': ticker,
                'apikey': 'demo'
            }

            response = requests.get(search_url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'bestMatches' in data and data['bestMatches']:
                    for match in data['bestMatches']:
                        symbol = match.get('1. symbol', '')
                        if symbol.upper() == ticker.upper():
                            print(f"Alpha VantageéªŒè¯æˆåŠŸ: {ticker}")
                            return True
        except:
            pass

        # æœ€åçš„å®½æ¾åˆ¤æ–­ - ä½†è¦æ›´åŠ è°¨æ…
        if len(ticker) >= 2 and len(ticker) <= 5 and ticker.isalpha():
            # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿tickerä¸æ˜¯æ˜æ˜¾çš„HTML/ç½‘é¡µå†…å®¹
            if not self.is_obviously_web_content(ticker):
                print(f"åŸºäºæ ¼å¼çš„è°¨æ…éªŒè¯: {ticker}")
                return True

        return False

    def is_obviously_web_content(self, ticker: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜æ˜¾æ˜¯ç½‘é¡µå†…å®¹"""
        web_content_patterns = [
            r'^(DIV|SPAN|HTML|HEAD|BODY|TITLE|META)$',
            r'^(HTTP|HTTPS|WWW|FTP)$',
            r'^(NEWS|INFO|HELP|HOME|MAIN|MENU)$',
            r'^(LOGIN|SIGNUP|REGISTER|SUBMIT)$',
            r'^(ABOUT|CONTACT|PRIVACY|TERMS)$'
        ]

        for pattern in web_content_patterns:
            if re.match(pattern, ticker.upper()):
                return True
        return False

    def search_delisted_stocks_online(self, company_name: str) -> Optional[Dict]:
        """ä¸“é—¨æœç´¢é€€å¸‚è‚¡ç¥¨ä¿¡æ¯"""
        print(f"æ­£åœ¨ç½‘ç»œæœç´¢é€€å¸‚è‚¡ç¥¨: {company_name}")

        try:
            # æ–¹æ³•1: ä½¿ç”¨Yahoo Financeå†å²æœç´¢
            result = self.search_yahoo_historical(company_name)
            if result:
                return result

            # æ–¹æ³•2: æœç´¢SEC EDGARæ•°æ®
            result = self.search_sec_edgar_enhanced(company_name)
            if result:
                return result

            # æ–¹æ³•3: ä½¿ç”¨æŠ•èµ„ç½‘ç«™æœç´¢
            result = self.search_investment_sites(company_name)
            if result:
                return result

            # æ–¹æ³•4: é€šç”¨ç½‘ç»œæœç´¢ï¼ˆæ”¹è¿›ç‰ˆï¼‰
            result = self.search_web_general_enhanced(company_name)
            if result:
                return result

        except Exception as e:
            print(f"é€€å¸‚è‚¡ç¥¨æœç´¢é”™è¯¯: {e}")

        return None

    def search_yahoo_historical(self, company_name: str) -> Optional[Dict]:
        """é€šè¿‡Yahoo Financeæœç´¢å†å²è‚¡ç¥¨"""
        try:
            # Yahoo Financeæœ‰æ—¶ä¼šä¿ç•™å†å²è‚¡ç¥¨ä¿¡æ¯
            search_url = "https://query1.finance.yahoo.com/v1/finance/search"
            params = {
                'q': company_name,
                'lang': 'en-US',
                'region': 'US',
                'quotesCount': 10,
                'newsCount': 0
            }

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = requests.get(search_url, params=params, headers=headers, timeout=10)

            if response.status_code == 200:
                data = response.json()
                if 'quotes' in data and data['quotes']:
                    for quote in data['quotes']:
                        symbol = quote.get('symbol', '')
                        quote_name = quote.get('longname', '') or quote.get('shortname', '')

                        if symbol and len(symbol) <= 5 and quote_name:
                            # éªŒè¯ç›¸ä¼¼åº¦
                            similarity = self.calculate_company_similarity(company_name, quote_name)
                            if similarity > 0.75:  # æé«˜é˜ˆå€¼
                                print(f"Yahoo Financeå†å²æœç´¢æ‰¾åˆ°: {symbol} (ç›¸ä¼¼åº¦: {similarity:.2f})")
                                return {
                                    'ticker': symbol,
                                    'company_name': company_name,
                                    'source': 'yahoo_historical',
                                    'quote_type': quote.get('typeDisp', 'Unknown'),
                                    'matched_name': quote_name,
                                    'similarity': similarity
                                }
        except Exception as e:
            print(f"Yahooå†å²æœç´¢é”™è¯¯: {e}")
        return None

    def search_sec_edgar_enhanced(self, company_name: str) -> Optional[Dict]:
        """å¢å¼ºçš„SEC EDGARæœç´¢"""
        try:
            print(f"æœç´¢SEC EDGAR: {company_name}")

            # SECæä¾›å…¬å¸æœç´¢API
            search_url = "https://www.sec.gov/cgi-bin/browse-edgar"
            params = {
                'company': company_name,
                'match': 'contains',
                'action': 'getcompany'
            }

            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; StockFinder/1.0; research-purpose)',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
            }

            response = requests.get(search_url, params=params, headers=headers, timeout=15)

            if response.status_code == 200:
                content = response.text

                # æå–å…¬å¸å…³é”®è¯
                company_keywords = self.extract_core_keywords(company_name)

                # æŸ¥æ‰¾Trading Symbolä¿¡æ¯
                symbol_pattern = r'Trading Symbol[:\s]*([A-Z]{1,5})'
                match = re.search(symbol_pattern, content, re.IGNORECASE)
                if match:
                    ticker = match.group(1)
                    # éªŒè¯ä¸Šä¸‹æ–‡
                    if self.validate_ticker_context(ticker, company_name, content):
                        print(f"SEC EDGARæ‰¾åˆ°éªŒè¯çš„è‚¡ç¥¨ä»£ç : {ticker}")
                        return {
                            'ticker': ticker,
                            'company_name': company_name,
                            'source': 'sec_edgar_enhanced'
                        }

                # æ›´ä¸¥æ ¼çš„æ›¿ä»£æ¨¡å¼æœç´¢
                ticker_patterns = [
                    r'symbol[:\s]+([A-Z]{2,5})',
                    r'ticker[:\s]+([A-Z]{2,5})',
                    r'NYSE[:\s]*([A-Z]{2,5})',
                    r'NASDAQ[:\s]*([A-Z]{2,5})'
                ]

                candidates = []
                for pattern in ticker_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches:
                        if len(match) >= 2 and len(match) <= 5:
                            # æ£€æŸ¥tickerå‘¨å›´æ˜¯å¦æœ‰å…¬å¸å…³é”®è¯
                            ticker_pos = content.upper().find(match)
                            if ticker_pos > 0:
                                context_start = max(0, ticker_pos - 200)
                                context_end = min(len(content), ticker_pos + 200)
                                context = content[context_start:context_end].upper()

                                keyword_found = any(keyword in context for keyword in company_keywords)
                                if keyword_found:
                                    candidates.append(match)

                # éªŒè¯å€™é€‰è‚¡ç¥¨ä»£ç 
                for candidate in candidates:
                    if self.validate_ticker_with_company_verification(candidate, company_name):
                        print(f"SEC EDGARéªŒè¯æˆåŠŸ: {candidate}")
                        return {
                            'ticker': candidate,
                            'company_name': company_name,
                            'source': 'sec_edgar_pattern'
                        }

        except Exception as e:
            print(f"SEC EDGARæœç´¢é”™è¯¯: {e}")
        return None

    def search_investment_sites(self, company_name: str) -> Optional[Dict]:
        """æœç´¢æŠ•èµ„ç½‘ç«™"""
        sites = [
            {
                'name': 'MarketWatch',
                'url': 'https://www.marketwatch.com/tools/quotes/lookup.asp',
                'params': {'Lookup': company_name, 'Country': 'us'},
                'pattern': r'symbol=([A-Z]{1,5})'
            }
        ]

        for site in sites:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }

                response = requests.get(site['url'], params=site['params'],
                                        headers=headers, timeout=10)

                if response.status_code == 200:
                    content = response.text
                    matches = re.findall(site['pattern'], content, re.IGNORECASE)

                    for match in matches:
                        if len(match) <= 5:
                            # éªŒè¯è‚¡ç¥¨ä»£ç 
                            if self.validate_ticker_with_company_verification(match, company_name):
                                print(f"{site['name']}æ‰¾åˆ°éªŒè¯çš„è‚¡ç¥¨ä»£ç : {match}")
                                return {
                                    'ticker': match,
                                    'company_name': company_name,
                                    'source': site['name'].lower()
                                }

                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

            except Exception as e:
                print(f"{site['name']}æœç´¢é”™è¯¯: {e}")
                continue

        return None

    def search_web_general_enhanced(self, company_name: str) -> Optional[Dict]:
        """å¢å¼ºçš„é€šç”¨ç½‘ç»œæœç´¢"""
        try:
            # æ„é€ æ›´ç²¾ç¡®çš„æœç´¢æŸ¥è¯¢
            search_queries = [
                f'"{company_name}" stock ticker symbol NYSE NASDAQ',
                f'"{company_name}" stock symbol trading',
                f'{company_name} ticker symbol exchange',
                f'"{company_name}" delisted stock ticker',
                f'{company_name} stock code symbol'
            ]

            for query in search_queries:
                print(f"ç½‘ç»œæœç´¢: {query}")

                # ä½¿ç”¨DuckDuckGoæœç´¢
                search_url = "https://html.duckduckgo.com/html/"
                params = {'q': query}

                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }

                try:
                    response = requests.get(search_url, params=params, headers=headers, timeout=15)
                    if response.status_code == 200:
                        content = response.text

                        # ä½¿ç”¨æ”¹è¿›çš„å€™é€‰æå–æ–¹æ³•
                        candidates = self.extract_ticker_candidates_enhanced(content, company_name)

                        # éªŒè¯æ¯ä¸ªå€™é€‰ - ä½¿ç”¨æ›´ä¸¥æ ¼çš„éªŒè¯
                        for candidate in candidates:
                            print(f"æ­£åœ¨éªŒè¯å€™é€‰è‚¡ç¥¨ä»£ç : {candidate}")

                            # é¦–å…ˆè¿›è¡ŒåŸºæœ¬æœ‰æ•ˆæ€§æ£€æŸ¥
                            if not self.is_valid_ticker(candidate):
                                continue

                            # è¿›è¡Œä¸Šä¸‹æ–‡éªŒè¯
                            if self.validate_ticker_context(candidate, company_name, content):
                                print(f"ä¸Šä¸‹æ–‡éªŒè¯é€šè¿‡: {candidate}")

                                # å°è¯•åœ¨çº¿éªŒè¯ï¼Œè¦æ±‚æ›´ä¸¥æ ¼
                                verification_result = self.validate_ticker_with_company_verification(candidate,
                                                                                                     company_name)
                                if verification_result:
                                    print(f"åœ¨çº¿éªŒè¯é€šè¿‡: {candidate}")
                                    return {
                                        'ticker': candidate,
                                        'company_name': company_name,
                                        'source': 'web_search_enhanced',
                                        'query': query,
                                        'verification': 'full'
                                    }
                                else:
                                    print(f"åœ¨çº¿éªŒè¯å¤±è´¥: {candidate}")
                                    # ä¸å†æ¥å—ä»…é€šè¿‡ä¸Šä¸‹æ–‡éªŒè¯çš„ç»“æœ

                    time.sleep(2)  # é¿å…è¢«å°IP

                except Exception as e:
                    print(f"æœç´¢æŸ¥è¯¢é”™è¯¯: {e}")
                    continue

        except Exception as e:
            print(f"å¢å¼ºç½‘ç»œæœç´¢é”™è¯¯: {e}")

        return None

    def extract_ticker_candidates_enhanced(self, content: str, company_name: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–è‚¡ç¥¨ä»£ç å€™é€‰ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        company_keywords = self.extract_core_keywords(company_name)
        candidates = set()

        # æœ€ç²¾ç¡®çš„åŒ¹é…æ¨¡å¼ - åªåŒ¹é…æ˜ç¡®çš„è‚¡ç¥¨ä»£ç æ ¼å¼
        precise_patterns = [
            # æ˜ç¡®çš„è‚¡ç¥¨ä»£ç æ ‡è¯†ï¼Œå¸¦å•è¯è¾¹ç•Œ
            r'(?:ticker|stock\s+symbol|trading\s+symbol)[\s:]+([A-Z]{2,5})(?:\s|$|[^\w])',
            # æ‹¬å·æ ¼å¼ï¼šå®Œæ•´çš„æ ¼å¼ (ä»£ç )
            r'\s\(([A-Z]{2,5})\)(?:\s|$|[^\w])',
            # NYSE/NASDAQ: ä»£ç æ ¼å¼ï¼Œç¡®ä¿ä¸åŒ¹é…äº¤æ˜“æ‰€åç§°æœ¬èº«
            r'(?:NYSE|NASDAQ)[\s:]+([A-Z]{2,5})(?:\s|$|[^\w])(?!.*(?:exchange|market|website))',
        ]

        # å¤„ç†ç²¾ç¡®æ¨¡å¼
        for pattern in precise_patterns:
            try:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if self.is_valid_ticker_strict(match, company_name, content):
                        candidates.add(match.upper())
                        print(f"ç²¾ç¡®åŒ¹é…æ‰¾åˆ°: {match}")
            except Exception as e:
                print(f"ç²¾ç¡®æ¨¡å¼åŒ¹é…é”™è¯¯: {e}")
                continue

        # å¦‚æœç²¾ç¡®åŒ¹é…æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä¸Šä¸‹æ–‡åŒ¹é…
        if not candidates:
            candidates = self.extract_contextual_candidates(content, company_name)

        # è¿‡æ»¤å’Œæ’åº
        filtered_candidates = []
        for candidate in candidates:
            if not self.is_obviously_invalid(candidate):
                # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„ä¸å…¬å¸ç›¸å…³
                if self.validate_candidate_relevance(candidate, company_name, content):
                    filtered_candidates.append(candidate)

        # æŒ‰ç›¸å…³åº¦æ’åºï¼šAGNç±»å‹çš„ä¼šæ’åœ¨å‰é¢
        sorted_candidates = sorted(filtered_candidates,
                                   key=lambda x: self.get_candidate_priority(x, company_name))

        print(f"è¿‡æ»¤åçš„å€™é€‰è‚¡ç¥¨ä»£ç : {sorted_candidates}")
        return sorted_candidates

    def extract_contextual_candidates(self, content: str, company_name: str) -> set:
        """åŸºäºä¸Šä¸‹æ–‡æå–å€™é€‰è‚¡ç¥¨ä»£ç """
        candidates = set()
        company_keywords = self.extract_core_keywords(company_name)

        # åœ¨å…¬å¸å…³é”®è¯é™„è¿‘æŸ¥æ‰¾è‚¡ç¥¨ä»£ç 
        for keyword in company_keywords:
            # æŸ¥æ‰¾å…³é”®è¯åœ¨å†…å®¹ä¸­çš„ä½ç½®
            keyword_positions = [m.start() for m in re.finditer(re.escape(keyword), content, re.IGNORECASE)]

            for pos in keyword_positions:
                # åœ¨å…³é”®è¯å‰å100ä¸ªå­—ç¬¦å†…æŸ¥æ‰¾è‚¡ç¥¨ä»£ç 
                start = max(0, pos - 100)
                end = min(len(content), pos + 100)
                context = content[start:end]

                # åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾è‚¡ç¥¨ä»£ç 
                context_patterns = [
                    r'\b([A-Z]{2,5})\b(?=\s*[:\-]|\s+(?:stock|shares|ticker|symbol))',
                    r'(?:ticker|symbol)[\s:]+([A-Z]{2,5})\b',
                    r'\(([A-Z]{2,5})\)',
                ]

                for pattern in context_patterns:
                    matches = re.findall(pattern, context)
                    for match in matches:
                        if self.is_valid_ticker_strict(match, company_name, context):
                            candidates.add(match.upper())
                            print(f"ä¸Šä¸‹æ–‡åŒ¹é…æ‰¾åˆ°: {match}")

        return candidates

    def is_valid_ticker_strict(self, ticker: str, company_name: str, content: str) -> bool:
        """ä¸¥æ ¼éªŒè¯è‚¡ç¥¨ä»£ç å€™é€‰"""
        if not ticker or not isinstance(ticker, str):
            return False

        ticker = ticker.upper()

        # åŸºæœ¬æ ¼å¼æ£€æŸ¥
        if not (2 <= len(ticker) <= 5) or not ticker.isalpha():
            return False

        # æ£€æŸ¥æ˜¯å¦æ˜æ˜¾æ— æ•ˆ
        if self.is_obviously_invalid(ticker):
            return False

        # æ£€æŸ¥æ˜¯å¦ä¸ºè¯­è¨€ä»£ç æˆ–å›½å®¶ä»£ç 
        if self.is_language_or_country_code(ticker):
            return False

        # æ£€æŸ¥æ˜¯å¦åœ¨è‚¡ç¥¨ç›¸å…³ä¸Šä¸‹æ–‡ä¸­
        if not self.is_in_strong_stock_context(ticker, content):
            return False

        return True

    def is_language_or_country_code(self, ticker: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè¯­è¨€ä»£ç æˆ–å›½å®¶ä»£ç """
        # å¸¸è§çš„è¯­è¨€ä»£ç 
        language_codes = {
            'EN', 'FR', 'DE', 'ES', 'IT', 'PT', 'NL', 'RU', 'ZH', 'JA', 'KO',
            'AR', 'HI', 'TR', 'PL', 'CS', 'HU', 'RO', 'BG', 'HR', 'SK', 'SL',
            'ET', 'LV', 'LT', 'MT', 'DA', 'SV', 'FI', 'NO', 'IS', 'GA', 'CY'
        }

        # å¸¸è§çš„å›½å®¶ä»£ç 
        country_codes = {
            'US', 'UK', 'CA', 'AU', 'NZ', 'IE', 'ZA', 'IN', 'CN', 'JP', 'KR',
            'BR', 'MX', 'AR', 'CL', 'PE', 'CO', 'VE', 'UY', 'PY', 'BO', 'EC',
            'FR', 'DE', 'ES', 'IT', 'PT', 'NL', 'BE', 'LU', 'CH', 'AT', 'SE',
            'DK', 'NO', 'FI', 'IS', 'IE', 'MT', 'CY', 'GR', 'BG', 'RO', 'HU',
            'CZ', 'SK', 'PL', 'SI', 'HR', 'EE', 'LV', 'LT'
        }

        return ticker in language_codes or ticker in country_codes

    def is_in_strong_stock_context(self, ticker: str, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åœ¨å¼ºè‚¡ç¥¨ä¸Šä¸‹æ–‡ä¸­ï¼ˆæ›´ä¸¥æ ¼çš„æ£€æŸ¥ï¼‰"""
        # å¼ºè‚¡ç¥¨ä¸Šä¸‹æ–‡æŒ‡ç¤ºè¯
        strong_stock_indicators = [
            'ticker symbol', 'stock symbol', 'trading symbol', 'stock ticker',
            'shares of', 'stock code', 'equity symbol', 'listed as',
            'trades as', 'symbol:', 'ticker:'
        ]

        # æŸ¥æ‰¾tickeråœ¨å†…å®¹ä¸­çš„ä½ç½®
        ticker_positions = [m.start() for m in re.finditer(re.escape(ticker), content, re.IGNORECASE)]

        for pos in ticker_positions:
            # æ£€æŸ¥å‰å50ä¸ªå­—ç¬¦
            start = max(0, pos - 50)
            end = min(len(content), pos + 50)
            context = content[start:end].lower()

            # å¿…é¡»åœ¨å¼ºè‚¡ç¥¨ä¸Šä¸‹æ–‡ä¸­
            if any(indicator in context for indicator in strong_stock_indicators):
                return True

            # æ£€æŸ¥æ˜¯å¦åœ¨æ‹¬å·æ ¼å¼ä¸­ï¼šå…¬å¸å (TICKER)
            if '(' in context and ')' in context:
                bracket_start = context.rfind('(', 0, pos - start)
                bracket_end = context.find(')', pos - start)
                if bracket_start >= 0 and bracket_end >= 0:
                    return True

        return False

    def validate_candidate_relevance(self, ticker: str, company_name: str, content: str) -> bool:
        """éªŒè¯å€™é€‰è‚¡ç¥¨ä»£ç ä¸å…¬å¸çš„ç›¸å…³æ€§"""
        company_keywords = self.extract_core_keywords(company_name)

        # æ£€æŸ¥tickerå‘¨å›´æ˜¯å¦æœ‰å…¬å¸å…³é”®è¯
        ticker_positions = [m.start() for m in re.finditer(re.escape(ticker), content, re.IGNORECASE)]

        for pos in ticker_positions:
            # åœ¨tickerå‰å150ä¸ªå­—ç¬¦å†…æŸ¥æ‰¾å…¬å¸å…³é”®è¯
            start = max(0, pos - 150)
            end = min(len(content), pos + 150)
            context = content[start:end].lower()

            # è®¡ç®—åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„å…¬å¸å…³é”®è¯æ•°é‡
            keyword_matches = sum(1 for keyword in company_keywords
                                  if keyword.lower() in context)

            # å¦‚æœæœ‰è¶³å¤Ÿçš„å…³é”®è¯åŒ¹é…ï¼Œè®¤ä¸ºç›¸å…³
            if keyword_matches >= max(1, len(company_keywords) // 2):
                return True

        return False

    def get_candidate_priority(self, ticker: str, company_name: str) -> int:
        """è·å–å€™é€‰è‚¡ç¥¨ä»£ç çš„ä¼˜å…ˆçº§ï¼ˆè¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰"""
        # AGNç±»å‹çš„ä»£ç ä¼˜å…ˆçº§æœ€é«˜
        if len(ticker) == 3:
            return 0
        # 4ä½ä»£ç æ¬¡ä¹‹
        elif len(ticker) == 4:
            return 1
        # 2ä½ä»£ç æœ€ä½ï¼ˆé€šå¸¸æ˜¯å›½å®¶ä»£ç ç­‰ï¼‰
        elif len(ticker) == 2:
            return 2
        # 5ä½ä»£ç 
        else:
            return 3

    def validate_ticker_context(self, ticker: str, company_name: str, content: str) -> bool:
        """éªŒè¯è‚¡ç¥¨ä»£ç ä¸å…¬å¸åç§°çš„å…³è”æ€§ - åŠ å¼ºç‰ˆ"""
        if not self.is_valid_ticker(ticker):
            return False

        # æå–å…¬å¸æ ¸å¿ƒå…³é”®è¯
        company_keywords = self.extract_core_keywords(company_name)

        # æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„å…¬å¸å…³é”®è¯
        content_lower = content.lower()

        # å¯¹äºçŸ­å…³é”®è¯æˆ–å¸¸è§è¯ï¼Œè¦æ±‚æ›´ä¸¥æ ¼çš„åŒ¹é…
        keyword_matches = 0
        for keyword in company_keywords:
            keyword_lower = keyword.lower()

            # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åœ¨å†…å®¹ä¸­
            if keyword_lower in content_lower:
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦åœ¨tickeré™„è¿‘ï¼ˆÂ±200å­—ç¬¦å†…ï¼‰
                ticker_positions = [m.start() for m in re.finditer(re.escape(ticker), content, re.IGNORECASE)]

                for pos in ticker_positions:
                    start = max(0, pos - 200)
                    end = min(len(content), pos + 200)
                    context = content[start:end].lower()

                    if keyword_lower in context:
                        keyword_matches += 1
                        break

        # æé«˜è¦æ±‚ï¼šè‡³å°‘è¦æœ‰ä¸€ä¸ªå…³é”®è¯åŒ¹é…ï¼Œæˆ–è€…å¯¹äºå•å…³é”®è¯å…¬å¸è¦æ±‚100%åŒ¹é…
        if len(company_keywords) == 1:
            required_matches = 1
        else:
            required_matches = max(1, len(company_keywords) // 2)  # è‡³å°‘1/2çš„å…³é”®è¯åŒ¹é…

        print(f"å…³é”®è¯åŒ¹é…æƒ…å†µ: {keyword_matches}/{len(company_keywords)} (éœ€è¦: {required_matches})")

        if keyword_matches >= required_matches:
            # é¢å¤–éªŒè¯ï¼šç¡®ä¿ä¸æ˜¯å·§åˆåŒ¹é…
            return self.validate_contextual_relationship(ticker, company_name, content)

        # å¤‡ç”¨æ£€æŸ¥ï¼šå¦‚æœtickerå’Œå…¬å¸ååœ¨åŒä¸€æ®µè½ä¸­å‡ºç°
        paragraphs = re.split(r'\n\s*\n', content)
        for paragraph in paragraphs:
            if ticker.upper() in paragraph.upper():
                # æ£€æŸ¥è¿™ä¸ªæ®µè½æ˜¯å¦åŒ…å«å…¬å¸å…³é”®è¯
                paragraph_matches = sum(1 for keyword in company_keywords
                                        if keyword.lower() in paragraph.lower())
                if paragraph_matches > 0:
                    print(f"æ®µè½çº§åŒ¹é…æˆåŠŸ: {paragraph_matches} ä¸ªå…³é”®è¯")
                    return self.validate_contextual_relationship(ticker, company_name, paragraph)

        return False

    def validate_contextual_relationship(self, ticker: str, company_name: str, content: str) -> bool:
        """éªŒè¯ä¸Šä¸‹æ–‡å…³ç³»çš„åˆç†æ€§"""
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„é”™è¯¯æŒ‡å‘
        misleading_patterns = [
            r'(?:not|no|incorrect|wrong|error|mistake)\s+(?:ticker|symbol)',
            r'(?:different|other|another)\s+(?:company|corporation)',
            r'(?:formerly|previously|old)\s+(?:known|called)'
        ]

        content_lower = content.lower()
        for pattern in misleading_patterns:
            if re.search(pattern, content_lower):
                print(f"å‘ç°è¯¯å¯¼æ€§æ¨¡å¼ï¼Œä¸Šä¸‹æ–‡éªŒè¯å¤±è´¥: {pattern}")
                return False

        # å¦‚æœæ²¡æœ‰å‘ç°é—®é¢˜ï¼Œåˆ™é€šè¿‡éªŒè¯
        return True

    def is_valid_ticker(self, ticker: str) -> bool:
        """éªŒè¯è‚¡ç¥¨ä»£ç æ˜¯å¦æœ‰æ•ˆ"""
        if not ticker or not isinstance(ticker, str):
            return False

        # åŸºæœ¬æ ¼å¼æ£€æŸ¥
        if not (1 <= len(ticker) <= 5):
            return False

        if not ticker.isupper() or not ticker.isalpha():
            return False

        # ä½¿ç”¨æ–°çš„æ£€æŸ¥æ–¹æ³•
        return not self.is_obviously_invalid(ticker)

    def is_obviously_invalid(self, ticker: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜æ˜¾ä¸æ˜¯è‚¡ç¥¨ä»£ç """
        ticker_upper = ticker.upper()

        # æ‰©å±•çš„æ’é™¤åˆ—è¡¨
        obvious_invalid = {
            # äº¤æ˜“æ‰€åç§°
            'NYSE', 'NASDAQ', 'NASDA', 'NASD', 'MKT',
            # ç½‘é¡µç›¸å…³
            'HTML', 'HTTP', 'HTTPS', 'WWW', 'COM', 'ORG', 'NET', 'GOV',
            # å¸¸è§è¯æ±‡
            'THE', 'AND', 'FOR', 'WITH', 'FROM', 'THIS', 'THAT',
            'MORE', 'ABOUT', 'CONTACT', 'NEWS', 'INFO', 'HELP',
            'PAGE', 'SITE', 'LINK', 'HREF', 'TEXT', 'FONT',
            # æœç´¢å¼•æ“ç›¸å…³
            'DDG', 'DUCK', 'GOOGLE', 'BING', 'YAHOO',
            # å…¬å¸åç¼€
            'INC', 'CORP', 'LTD', 'LLC', 'PLC',
            # åœ°ç†ä½ç½®
            'USA', 'US', 'UK', 'CA', 'NY', 'IE', 'EU',
            # å…¶ä»–å¸¸è§è¯¯åŒ¹é…
            'NEWS', 'HOME', 'MAIN', 'MENU', 'SEARCH', 'LOGIN',
            'MSN', 'CNN', 'BBC', 'ABC', 'CBS', 'NBC',
            # ç½‘é¡µå†…å®¹ç›¸å…³ï¼ˆæ–°å¢ï¼‰
            'LINK', 'HREF', 'SRC', 'ALT', 'DIV', 'SPAN'
        }

        if ticker_upper in obvious_invalid:
            return True

        # æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥åƒç½‘ç«™åŸŸåçš„ä¸€éƒ¨åˆ†
        if ticker_upper.endswith('COM') or ticker_upper.endswith('NET'):
            return True

        # æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥åƒHTMLæ ‡ç­¾
        html_patterns = [
            r'^H[1-6]$',  # HTMLæ ‡é¢˜
            r'^(BR|HR|TD|TR|TH|LI|UL|OL)$',  # HTMLæ ‡ç­¾
            r'^(DIV|SPAN|FONT|BOLD)$',  # HTMLå®¹å™¨
            r'^(SRC|ALT|REF|REL)$'  # HTMLå±æ€§
        ]

        for pattern in html_patterns:
            if re.match(pattern, ticker_upper):
                return True

        return False

    def find_ticker(self, company_name: str, use_online: bool = True) -> Optional[Dict]:
        """
        æŸ¥æ‰¾å…¬å¸çš„è‚¡ç¥¨ä»£ç  - å®Œå…¨ä½¿ç”¨test3.pyçš„é€»è¾‘
        """
        if not company_name or pd.isna(company_name):
            return None

        print(f"\næ­£åœ¨æœç´¢: {company_name}")

        # 1. é¦–å…ˆåœ¨æœ¬åœ°æ•°æ®ä¸­æœç´¢ - å®Œå…¨ä½¿ç”¨test3.pyçš„é€»è¾‘
        local_results = self.search_local(company_name)
        if local_results:
            best_match = local_results[0]
            company_info = best_match[0]
            similarity = best_match[1]

            print(f"æœ¬åœ°åŒ¹é…æ‰¾åˆ°: {company_info['name']} -> {company_info['ticker']}")
            return {
                'ticker': company_info['ticker'],
                'company_name': company_info['name'],
                'similarity': similarity,
                'source': 'local',
                'status': 'active'
            }

        # 2. åœ¨çº¿æœç´¢
        if use_online:
            print("å¼€å§‹åœ¨çº¿æœç´¢...")
            delisted_result = self.search_delisted_stocks_online(company_name)
            if delisted_result:
                ticker = delisted_result['ticker']
                print(f"åœ¨çº¿æœç´¢æ‰¾åˆ°: {company_name} -> {ticker}")

                return {
                    'ticker': ticker,
                    'company_name': company_name,
                    'source': delisted_result['source'],
                    'status': delisted_result.get('status', 'unknown'),
                    'search_method': delisted_result.get('query', 'multiple_methods'),
                    'similarity': delisted_result.get('similarity')
                }

            print("æ ‡å‡†åœ¨çº¿æœç´¢æœªæ‰¾åˆ°ç»“æœ")

        print("æœªæ‰¾åˆ°åŒ¹é…çš„è‚¡ç¥¨ä»£ç ")
        return None

    def convert_csv_files(self, csv_pattern="*_all_quarters_merged.csv"):
        """
        è½¬æ¢æ‰€æœ‰åŒ¹é…æ¨¡å¼çš„CSVæ–‡ä»¶ - ä½¿ç”¨å®Œå…¨åŒ¹é…test3.pyçš„é€»è¾‘
        """
        print("=" * 60)
        print("å¼€å§‹CSVæ–‡ä»¶è½¬æ¢ - ä½¿ç”¨å®Œå…¨åŒ¹é…test3.pyçš„é€»è¾‘...")

        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„CSVæ–‡ä»¶
        current_dir = Path('.')
        csv_files = list(current_dir.glob(csv_pattern))

        if not csv_files:
            print(f"æœªæ‰¾åˆ°åŒ¹é…æ¨¡å¼ '{csv_pattern}' çš„CSVæ–‡ä»¶")
            return

        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶éœ€è¦å¤„ç†")

        # ç»Ÿè®¡ä¿¡æ¯
        total_processed = 0
        total_matched = 0
        all_unmatched = set()

        for csv_file in csv_files:
            print(f"\nå¤„ç†æ–‡ä»¶: {csv_file}")

            try:
                # è¯»å–CSVæ–‡ä»¶
                df = pd.read_csv(csv_file)

                # æŸ¥æ‰¾nameOfIssueråˆ—
                name_column = None
                for col in df.columns:
                    if 'nameOfIssuer' in col or 'nameOfIssue' in col:
                        name_column = col
                        break

                if name_column is None:
                    print(f"  è­¦å‘Š: æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'nameOfIssuer' æˆ– 'nameOfIssue' åˆ—")
                    print(f"  å¯ç”¨åˆ—: {list(df.columns)}")
                    continue

                # åˆ›å»ºæ–°çš„DataFrameï¼ŒåŒ…å«å¿…éœ€çš„åˆ—: nameOfIssuer, Symbol, Source
                result_df = pd.DataFrame()

                tickers = []
                sources = []
                matched_count = 0

                print(f"  å¤„ç† {len(df)} å®¶å…¬å¸...")

                for i, company_name in enumerate(df[name_column]):
                    if i % 10 == 0 and i > 0:
                        print(f"    è¿›åº¦: {i}/{len(df)} ({i / len(df) * 100:.1f}%)")

                    result = self.find_ticker(company_name, use_online=True)

                    if result:
                        tickers.append(result['ticker'])
                        sources.append(result['source'])
                        matched_count += 1
                    else:
                        tickers.append(None)
                        sources.append('not_found')
                        if pd.notna(company_name) and str(company_name).strip():
                            all_unmatched.add(str(company_name).strip())

                # åˆ›å»ºç»“æœDataFrame: nameOfIssuer, Symbol, Source
                result_df['nameOfIssuer'] = df[name_column].copy()
                result_df['Symbol'] = tickers
                result_df['Source'] = sources

                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                output_file = csv_file.stem + '_with_tickers.csv'

                # ä¿å­˜ç»“æœ
                result_df.to_csv(output_file, index=False)

                total_processed += len(df)
                total_matched += matched_count

                print(f"  âœ… å¤„ç†å®Œæˆ: {output_file}")
                print(f"  ğŸ“Š æ€»è®¡ {len(df)} æ¡è®°å½•ï¼ŒæˆåŠŸåŒ¹é… {matched_count} ä¸ªè‚¡ç¥¨ä»£ç  ({matched_count / len(df) * 100:.1f}%)")

                # æ˜¾ç¤ºä¸€äº›åŒ¹é…ç¤ºä¾‹
                matched_examples = result_df[result_df['Symbol'].notna()].head(3)
                if not matched_examples.empty:
                    print(f"  ğŸ¯ åŒ¹é…ç¤ºä¾‹:")
                    for _, row in matched_examples.iterrows():
                        print(f"     '{row['nameOfIssuer']}' â†’ {row['Symbol']} ({row['Source']})")

            except Exception as e:
                print(f"  âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        print(f"\n" + "=" * 60)
        print(f"ğŸ“ˆ å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"   æ€»å¤„ç†è®°å½•æ•°: {total_processed:,}")
        print(f"   æˆåŠŸåŒ¹é…æ•°: {total_matched:,}")
        print(f"   æ€»ä½“åŒ¹é…ç‡: {total_matched / total_processed * 100:.1f}%")

        # æ˜¾ç¤ºä¸€äº›æœªåŒ¹é…çš„å…¬å¸
        if all_unmatched:
            print(f"\nâ“ æœªåŒ¹é…å…¬å¸ç¤ºä¾‹ (æ€»è®¡ {len(all_unmatched)} å®¶):")
            sorted_unmatched = sorted(all_unmatched)
            for i, name in enumerate(sorted_unmatched[:15]):
                print(f"   {i + 1:2d}. {name}")
            if len(all_unmatched) > 15:
                print(f"   ... è¿˜æœ‰ {len(all_unmatched) - 15} å®¶æœªæ˜¾ç¤º")

        print(f"\nğŸ’¡ æ³¨æ„: ä½¿ç”¨å®Œå…¨åŒ¹é…test3.pyçš„é€»è¾‘")
        print(f"   - ç›¸åŒçš„æ•°æ®åŠ è½½é€»è¾‘")
        print(f"   - ç›¸åŒçš„ç›¸ä¼¼åº¦è®¡ç®—ç®—æ³•")
        print(f"   - ç›¸åŒçš„éªŒè¯æœºåˆ¶å’Œé˜ˆå€¼")
        print(f"   - ç›¸åŒçš„æœç´¢å’Œè¿‡æ»¤é€»è¾‘")


def check_required_files():
    """æ£€æŸ¥æ‰€éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    required_files = ["company_tickers.json", "company_tickers_exchange.json"]
    missing_files = []

    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)

    if missing_files:
        print("âŒ ç¼ºå°‘å¿…éœ€æ–‡ä»¶:")
        for file in missing_files:
            print(f"   - {file}")
        print("\nğŸ“¥ è¯·ä»ä»¥ä¸‹åœ°å€ä¸‹è½½:")
        print("   - company_tickers.json: https://www.sec.gov/files/company_tickers.json")
        print("   - company_tickers_exchange.json: https://www.sec.gov/files/company_tickers_exchange.json")
        return False

    print("âœ… æ£€æµ‹åˆ°å¿…éœ€çš„SECæ–‡ä»¶:")
    for file in required_files:
        print(f"   - {file}")

    return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Enhanced Company Name to Ticker Tool (FIXED - å®Œå…¨åŒ¹é…test3.pyé€»è¾‘)")
    print("=" * 60)

    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    if not check_required_files():
        return

    # åˆå§‹åŒ–è½¬æ¢å™¨
    converter = EnhancedTest3TickerConverter()

    # è½¬æ¢CSVæ–‡ä»¶
    converter.convert_csv_files()

    print("\n" + "=" * 60)
    print("ğŸ‰ è½¬æ¢å®Œæˆ!")
    print("\nğŸ“‹ ä¿®å¤å®Œæˆçš„ç‰¹æ€§:")
    print("âœ… å®Œå…¨ä½¿ç”¨test3.pyçš„æ•°æ®åŠ è½½é€»è¾‘")
    print("âœ… å®Œå…¨ä½¿ç”¨test3.pyçš„ç›¸ä¼¼åº¦è®¡ç®—ç®—æ³•")
    print("âœ… å®Œå…¨ä½¿ç”¨test3.pyçš„æœç´¢å’ŒéªŒè¯é€»è¾‘")
    print("âœ… ç›¸åŒçš„é˜ˆå€¼å’Œè¿‡æ»¤æ ‡å‡†")
    print("âœ… è¾“å‡ºæ ¼å¼: nameOfIssuer, Symbol, Source")
    print("âœ… æ•°æ®æº: local/yahoo_historical/sec_edgar_enhanced/web_search_enhanced/not_found")
    print("\nğŸ¯ ç°åœ¨åº”è¯¥èƒ½å¤Ÿæ‰¾åˆ° 'E M C CORP MASS' â†’ 'EMC' äº†!")


if __name__ == "__main__":
    main()